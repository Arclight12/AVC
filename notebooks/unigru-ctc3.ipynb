{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# sensor_to_phoneme_ctc_fixed.py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom collections import defaultdict\nimport random\nimport os\n\n# -----------------------\n# 0. CONFIG\n# -----------------------\nMAX_SEQUENCE_LENGTH = 400\nRAW_FEATURE_DIM = 5\nFEATURE_DIM = 7  # 5 raw + 2 ratios\nNUM_PHONEMES = 40\nBLANK_INDEX = 39\nBATCH_SIZE = 16\nNUM_EPOCHS = 50\nLEARNING_RATE = 3e-4\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnp.random.seed(42)\ntorch.manual_seed(42)\nrandom.seed(42)\n\n# -----------------------\n# 1. ARPAbet\n# -----------------------\narpabet = {\n    \"AA\":0, \"AE\":1, \"AH\":2, \"AO\":3, \"AW\":4, \"AY\":5, \"B\":6, \"CH\":7, \"D\":8, \"DH\":9,\n    \"EH\":10, \"ER\":11, \"EY\":12, \"F\":13, \"G\":14, \"HH\":15, \"IH\":16, \"IY\":17, \"JH\":18,\n    \"K\":19, \"L\":20, \"M\":21, \"N\":22, \"NG\":23, \"OW\":24, \"OY\":25, \"P\":26, \"R\":27,\n    \"S\":28, \"SH\":29, \"T\":30, \"TH\":31, \"UH\":32, \"UW\":33, \"V\":34, \"W\":35, \"Y\":36,\n    \"Z\":37, \"ZH\":38, \"<BLANK>\":39\n}\nindex_to_ph = {v: k for k, v in arpabet.items()}\n\n# -----------------------\n# 2. SENTENCES + PHONEMES\n# -----------------------\nSENTENCES = [\n    \"WHAT IS YOUR NAME\", \"WHERE ARE YOU FROM\", \"HOW ARE YOU\", \"NICE TO MEET YOU\",\n    \"CAN YOU HELP ME\", \"THANK YOU VERY MUCH\", \"GOOD MORNING\", \"GOOD NIGHT\",\n    \"SEE YOU LATER\", \"I AM FINE\", \"I LOVE YOU\", \"PLEASE WAIT\", \"EXCUSE ME\",\n    \"SORRY ABOUT THAT\", \"I DON'T KNOW\", \"CAN YOU REPEAT\", \"TURN LEFT\", \"TURN RIGHT\",\n    \"GO STRAIGHT\", \"STOP HERE\", \"I NEED WATER\", \"I AM HUNGRY\", \"I AM TIRED\",\n    \"CALL THE DOCTOR\", \"OPEN THE DOOR\", \"CLOSE THE WINDOW\", \"TURN ON THE LIGHT\",\n    \"TURN OFF THE FAN\", \"PLAY MUSIC\", \"STOP THE MUSIC\", \"WHAT TIME IS IT\",\n    \"TODAY IS MONDAY\", \"TOMORROW IS TUESDAY\", \"I WANT COFFEE\", \"THIS IS DELICIOUS\",\n    \"HOW MUCH IS THIS\", \"I WILL PAY\", \"KEEP THE CHANGE\", \"SEE YOU TOMORROW\",\n    \"HAVE A GOOD DAY\", \"TAKE CARE\", \"BE CAREFUL\", \"DON'T WORRY\", \"I UNDERSTAND\",\n    \"I DON'T UNDERSTAND\", \"SPEAK SLOWLY\", \"WRITE IT DOWN\", \"SHOW ME AGAIN\",\n    \"ONE MORE TIME\", \"YES PLEASE\", \"NO THANKS\"\n]\n\nSENTENCE_PHONEMES = {\n    \"WHAT IS YOUR NAME\": [\"W\",\"AH\",\"T\",\"IH\",\"Z\",\"Y\",\"AO\",\"R\",\"N\",\"EY\",\"M\"],\n    \"WHERE ARE YOU FROM\": [\"W\",\"EH\",\"R\",\"AA\",\"R\",\"Y\",\"UW\",\"F\",\"R\",\"AH\",\"M\"],\n    \"HOW ARE YOU\": [\"HH\",\"AW\",\"AA\",\"R\",\"Y\",\"UW\"],\n    \"NICE TO MEET YOU\": [\"N\",\"AY\",\"S\",\"T\",\"UW\",\"M\",\"IY\",\"T\",\"Y\",\"UW\"],\n    \"CAN YOU HELP ME\": [\"K\",\"AE\",\"N\",\"Y\",\"UW\",\"HH\",\"EH\",\"L\",\"P\",\"M\",\"IY\"],\n    \"THANK YOU VERY MUCH\": [\"TH\",\"AE\",\"NG\",\"K\",\"Y\",\"UW\",\"V\",\"EH\",\"R\",\"IY\",\"M\",\"AH\",\"CH\"],\n    \"GOOD MORNING\": [\"G\",\"UH\",\"D\",\"M\",\"AO\",\"R\",\"N\",\"IH\",\"NG\"],\n    \"GOOD NIGHT\": [\"G\",\"UH\",\"D\",\"N\",\"AY\",\"T\"],\n    \"SEE YOU LATER\": [\"S\",\"IY\",\"Y\",\"UW\",\"L\",\"EY\",\"T\",\"ER\"],\n    \"I AM FINE\": [\"AY\",\"AE\",\"M\",\"F\",\"AY\",\"N\"],\n    \"I LOVE YOU\": [\"AY\",\"L\",\"AH\",\"V\",\"Y\",\"UW\"],\n    \"PLEASE WAIT\": [\"P\",\"L\",\"IY\",\"Z\",\"W\",\"EY\",\"T\"],\n    \"EXCUSE ME\": [\"IH\",\"K\",\"S\",\"K\",\"Y\",\"UW\",\"Z\",\"M\",\"IY\"],\n    \"SORRY ABOUT THAT\": [\"S\",\"AO\",\"R\",\"IY\",\"AH\",\"B\",\"AW\",\"T\",\"DH\",\"AE\",\"T\"],\n    \"I DON'T KNOW\": [\"AY\",\"D\",\"OW\",\"N\",\"T\",\"N\",\"OW\"],\n    \"CAN YOU REPEAT\": [\"K\",\"AE\",\"N\",\"Y\",\"UW\",\"R\",\"IH\",\"P\",\"IY\",\"T\"],\n    \"TURN LEFT\": [\"T\",\"ER\",\"N\",\"L\",\"EH\",\"F\",\"T\"],\n    \"TURN RIGHT\": [\"T\",\"ER\",\"N\",\"R\",\"AY\",\"T\"],\n    \"GO STRAIGHT\": [\"G\",\"OW\",\"S\",\"T\",\"R\",\"EY\",\"T\"],\n    \"STOP HERE\": [\"S\",\"T\",\"AA\",\"P\",\"HH\",\"IY\",\"R\"],\n    \"I NEED WATER\": [\"AY\",\"N\",\"IY\",\"D\",\"W\",\"AO\",\"T\",\"ER\"],\n    \"I AM HUNGRY\": [\"AY\",\"AE\",\"M\",\"HH\",\"AH\",\"NG\",\"G\",\"R\",\"IY\"],\n    \"I AM TIRED\": [\"AY\",\"AE\",\"M\",\"T\",\"AY\",\"ER\",\"D\"],\n    \"CALL THE DOCTOR\": [\"K\",\"AO\",\"L\",\"DH\",\"AH\",\"D\",\"AA\",\"K\",\"T\",\"ER\"],\n    \"OPEN THE DOOR\": [\"OW\",\"P\",\"AH\",\"N\",\"DH\",\"AH\",\"D\",\"AO\",\"R\"],\n    \"CLOSE THE WINDOW\": [\"K\",\"L\",\"OW\",\"Z\",\"DH\",\"AH\",\"W\",\"IH\",\"N\",\"D\",\"OW\"],\n    \"TURN ON THE LIGHT\": [\"T\",\"ER\",\"N\",\"AA\",\"N\",\"DH\",\"AH\",\"L\",\"AY\",\"T\"],\n    \"TURN OFF THE FAN\": [\"T\",\"ER\",\"N\",\"AO\",\"F\",\"DH\",\"AH\",\"F\",\"AE\",\"N\"],\n    \"PLAY MUSIC\": [\"P\",\"L\",\"EY\",\"M\",\"Y\",\"UW\",\"Z\",\"IH\",\"K\"],\n    \"STOP THE MUSIC\": [\"S\",\"T\",\"AA\",\"P\",\"DH\",\"AH\",\"M\",\"Y\",\"UW\",\"Z\",\"IH\",\"K\"],\n    \"WHAT TIME IS IT\": [\"W\",\"AH\",\"T\",\"T\",\"AY\",\"M\",\"IH\",\"Z\",\"IH\",\"T\"],\n    \"TODAY IS MONDAY\": [\"T\",\"UW\",\"D\",\"EY\",\"IH\",\"Z\",\"M\",\"AH\",\"N\",\"D\",\"EY\"],\n    \"TOMORROW IS TUESDAY\": [\"T\",\"AH\",\"M\",\"AA\",\"R\",\"OW\",\"IH\",\"Z\",\"T\",\"UW\",\"Z\",\"D\",\"EY\"],\n    \"I WANT COFFEE\": [\"AY\",\"W\",\"AA\",\"N\",\"T\",\"K\",\"AO\",\"F\",\"IY\"],\n    \"THIS IS DELICIOUS\": [\"DH\",\"IH\",\"S\",\"IH\",\"Z\",\"D\",\"IH\",\"L\",\"IH\",\"SH\",\"AH\",\"S\"],\n    \"HOW MUCH IS THIS\": [\"HH\",\"AW\",\"M\",\"AH\",\"CH\",\"IH\",\"Z\",\"DH\",\"IH\",\"S\"],\n    \"I WILL PAY\": [\"AY\",\"W\",\"IH\",\"L\",\"P\",\"EY\"],\n    \"KEEP THE CHANGE\": [\"K\",\"IY\",\"P\",\"DH\",\"AH\",\"CH\",\"EY\",\"N\",\"JH\"],\n    \"SEE YOU TOMORROW\": [\"S\",\"IY\",\"Y\",\"UW\",\"T\",\"AH\",\"M\",\"AA\",\"R\",\"OW\"],\n    \"HAVE A GOOD DAY\": [\"HH\",\"AE\",\"V\",\"AH\",\"G\",\"UH\",\"D\",\"D\",\"EY\"],\n    \"TAKE CARE\": [\"T\",\"EY\",\"K\",\"K\",\"EH\",\"R\"],\n    \"BE CAREFUL\": [\"B\",\"IY\",\"K\",\"EH\",\"R\",\"F\",\"AH\",\"L\"],\n    \"DON'T WORRY\": [\"D\",\"OW\",\"N\",\"T\",\"W\",\"ER\",\"IY\"],\n    \"I UNDERSTAND\": [\"AY\",\"AH\",\"N\",\"D\",\"ER\",\"S\",\"T\",\"AE\",\"N\",\"D\"],\n    \"I DON'T UNDERSTAND\": [\"AY\",\"D\",\"OW\",\"N\",\"T\",\"AH\",\"N\",\"D\",\"ER\",\"S\",\"T\",\"AE\",\"N\",\"D\"],\n    \"SPEAK SLOWLY\": [\"S\",\"P\",\"IY\",\"K\",\"S\",\"L\",\"OW\",\"L\",\"IY\"],\n    \"WRITE IT DOWN\": [\"R\",\"AY\",\"T\",\"IH\",\"T\",\"D\",\"AW\",\"N\"],\n    \"SHOW ME AGAIN\": [\"SH\",\"OW\",\"M\",\"IY\",\"AH\",\"G\",\"EH\",\"N\"],\n    \"ONE MORE TIME\": [\"W\",\"AH\",\"N\",\"M\",\"AO\",\"R\",\"T\",\"AY\",\"M\"],\n    \"YES PLEASE\": [\"Y\",\"EH\",\"S\",\"P\",\"L\",\"IY\",\"Z\"],\n    \"NO THANKS\": [\"N\",\"OW\",\"TH\",\"AE\",\"NG\",\"K\",\"S\"],\n}\n\n# -----------------------\n# 3. manner + generator (unchanged logic)\n# -----------------------\ndef get_manner(idx):\n    stops = [6,8,19,26,30]; affricates = [7,18]; frics = [13,14,28,29,34,37,38]\n    nasals = [21,22,23]; liquids = [20,27]; glides = [15,35,36]\n    if idx in stops: return 0\n    if idx in affricates: return 1\n    if idx in frics: return 2\n    if idx in nasals: return 3\n    if idx in liquids: return 4\n    if idx in glides: return 5\n    return 6\n\ndef smooth_envelope(n):\n    x = np.linspace(0, np.pi, n)\n    return 0.7 + 0.3 * (1 - np.cos(x)) / 2\n\ndef generate_sensor_sequence(sentence):\n    phonemes = SENTENCE_PHONEMES[sentence]\n    ids = [arpabet[p] for p in phonemes]\n    seq = []\n\n    for idx in ids:\n        manner = get_manner(idx)\n        bases = [\n            [1.3, 0.9, 0.1, 0.2, 0.8], [0.9, 0.2, 0.9, 1.0, 0.4], [0.8, 0.2, 0.9, 0.9, 0.3],\n            [0.5, 0.3, 1.0, 0.3, 0.2], [0.6, 0.7, 0.6, 0.4, 0.6], [0.4, 0.8, 0.4, 0.5, 0.7],\n            [0.7, 0.5, 0.5, 0.8, 0.4]\n        ]\n        base = np.array(bases[manner])\n        base += np.random.normal(0, 0.15, 5)\n\n        duration = np.random.randint(4, 13)\n        t_scale = np.random.uniform(0.9, 1.1)\n        t = np.linspace(0, 2*np.pi * t_scale, duration)\n        env = smooth_envelope(duration) * np.random.uniform(0.7, 1.3)\n\n        f1 = 1.5 + 0.8*manner + np.random.uniform(-0.5, 0.5)\n        f2 = 2.5 + 0.6*manner + np.random.uniform(-0.7, 0.7)\n\n        seg = np.zeros((duration, RAW_FEATURE_DIM))\n        seg[:, 0] = base[0] + np.random.normal(0, 0.08, duration)\n        seg[:, 1] = base[1] + np.random.normal(0, 0.08, duration)\n        seg[:, 2] = base[2] + np.random.normal(0, 0.08, duration)\n        seg[:, 3] = base[3] * np.sin(t * f1) * env + np.random.normal(0, 0.05, duration)\n        seg[:, 4] = base[4] * np.sin(t * f2) * env + np.random.normal(0, 0.05, duration)\n\n        seq.append(seg)\n\n        if len(seq) > 1:\n            prev = seq[-2]; curr = seg\n            blend = np.random.randint(1, 4)\n            if blend < len(prev) and blend < len(curr):\n                prev[-blend:] = np.random.uniform(0.4, 0.7) * prev[-blend:] + (1 - np.random.uniform(0.4, 0.7)) * curr[:blend]\n\n    full = np.concatenate(seq, axis=0)\n    full += np.random.normal(0, 0.1, full.shape)\n    drift = 0.05 * np.sin(np.linspace(0, 2*np.pi, len(full)))\n    full *= (1 + drift[:, np.newaxis])\n    full = np.clip(full, -3, 3)\n\n    L = min(len(full), MAX_SEQUENCE_LENGTH)\n    return full[:L], np.array(ids), L\n\n# -----------------------\n# 4. Build dataset (7 features) and normalize\n# -----------------------\nprint(\"Generating dataset (5000 utterances)...\")\nX_list, Y_list, L_list = [], [], []\n\nfor _ in range(5000):\n    sent = random.choice(SENTENCES)\n    X_raw, Y_ids, L = generate_sensor_sequence(sent)  # (T,5)\n    airflow = X_raw[:, 0]\n    accel = X_raw[:, 1] + 1e-6\n    moisture = X_raw[:, 2]\n    r1 = airflow / accel\n    r2 = moisture / (airflow + 1e-6)\n    X_full = np.concatenate([X_raw, r1[:,None], r2[:,None]], axis=1)  # (T,7)\n    X_pad = np.pad(X_full, ((0, MAX_SEQUENCE_LENGTH - L), (0, 0)), 'constant')\n    X_list.append(X_pad.astype(np.float32))\n    Y_list.append(Y_ids.astype(np.int64))\n    L_list.append(L)\n\n# Convert to tensor for normalization\nX_tensor = torch.tensor(np.stack(X_list, axis=0), dtype=torch.float32)  # (N, T, F)\ninput_lengths = torch.tensor(L_list, dtype=torch.long)\n\n# compute mean/std using valid frames only\nwith torch.no_grad():\n    mask = torch.arange(MAX_SEQUENCE_LENGTH)[None,:] < input_lengths[:,None]\n    denom = mask.sum().item() + 1e-8\n    mean = (X_tensor * mask.unsqueeze(2)).sum(dim=(0,1)) / denom\n    std = torch.sqrt(((X_tensor - mean)**2 * mask.unsqueeze(2)).sum(dim=(0,1)) / denom + 1e-6)\n    X_tensor = (X_tensor - mean) / std\n    X_tensor[~mask] = 0.0\n\n# Create dataset as list of tuples (tensor, target_array, original_length, target_len)\ndataset = []\nfor i in range(len(X_list)):\n    dataset.append((X_tensor[i], Y_list[i], L_list[i], len(Y_list[i])))\n\n# -----------------------\n# 5. Model (keeps your arch)\n# -----------------------\nclass SensorToPhoneme(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.proj = nn.Linear(FEATURE_DIM, 512)\n        self.pool = nn.AvgPool1d(4, 4)   # reduces time dim by 4\n        self.gru = nn.GRU(512, 512, 4, batch_first=True, dropout=0.3)\n        self.out = nn.Sequential(nn.Linear(512, NUM_PHONEMES), nn.LogSoftmax(dim=2))\n\n    def forward(self, x):\n        # x: (B, T, F)\n        x = torch.tanh(self.proj(x))    # (B, T, 512)\n        x = x.permute(0, 2, 1)          # (B, 512, T)\n        x = self.pool(x)                # (B, 512, T//4)\n        x = x.permute(0, 2, 1)          # (B, T//4, 512)\n        x, _ = self.gru(x)              # (B, T//4, 512)\n        x = self.out(x)                 # (B, T//4, C)\n        return x.transpose(0, 1)        # (T//4, B, C) -> CTCLoss expects (T, N, C)\n\nmodel = SensorToPhoneme().to(device)\nctc = nn.CTCLoss(blank=BLANK_INDEX, zero_infinity=True)\nopt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# -----------------------\n# 6. collate_fn & DataLoader\n# -----------------------\ndef collate_fn(batch):\n    \"\"\"\n    batch: list of tuples (X_tensor (T,F), Y_array (variable), L, target_len)\n    Return:\n      Xs: (B, MAX_SEQUENCE_LENGTH, F) tensor\n      targets_batch: 1D long tensor (concatenated targets for this batch)\n      input_lens: original lengths (before pooling)\n      target_lens: lengths of each target sequence\n    \"\"\"\n    Xs = torch.stack([item[0] for item in batch], dim=0)  # (B, T, F)\n    input_lens = torch.tensor([item[2] for item in batch], dtype=torch.long)\n    target_lens = torch.tensor([item[3] for item in batch], dtype=torch.long)\n    targets_batch = torch.cat([torch.tensor(item[1], dtype=torch.long) for item in batch], dim=0)\n    return Xs, targets_batch, input_lens, target_lens\n\nloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, drop_last=False, num_workers=0)\n\n# -----------------------\n# 7. Greedy decoder (phoneme output)\n# -----------------------\ndef greedy_decode(log_probs, input_lens_reduced):\n    \"\"\"\n    log_probs: (T, N, C) tensor (log-probabilities)\n    input_lens_reduced: tensor of reduced input lengths (after pooling) - length N\n    Returns: list of lists of ARPAbet phoneme symbols\n    \"\"\"\n    # get predictions (T, N)\n    with torch.no_grad():\n        preds = log_probs.argmax(dim=2)  # (T, N)\n        preds = preds.transpose(0,1).cpu().numpy()  # (N, T)\n    results = []\n    for i, Lr in enumerate(input_lens_reduced.cpu().numpy()):\n        seq = preds[i, :Lr].tolist()\n        out = []\n        prev = -1\n        for s in seq:\n            if s != prev and s != BLANK_INDEX:\n                out.append(index_to_ph.get(int(s), \"?\"))\n            prev = s\n        results.append(out)\n    return results\n\n# -----------------------\n# 8. TRAIN\n# -----------------------\nprint(\"\\nTRAINING...\")\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    total_loss = 0.0\n    batches = 0\n    for Xs, targets_batch, input_lens_batch, target_lens_batch in loader:\n        Xs = Xs.to(device)                      # (B, T, F)\n        targets_batch = targets_batch.to(device) # 1D\n        # reduced lengths after AvgPool1d(4,4)\n        input_lens_reduced = (input_lens_batch // 4).clamp(min=1).to(device)\n        target_lens_batch = target_lens_batch.to(device)\n\n        opt.zero_grad()\n        logp = model(Xs)  # (T_reduced, B, C)\n        try:\n            loss = ctc(logp, targets_batch, input_lens_reduced, target_lens_batch)\n        except Exception as e:\n            print(\"CTC error:\", e)\n            continue\n\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(\"Skipping batch due to NaN/Inf loss\")\n            continue\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n\n        total_loss += loss.item()\n        batches += 1\n\n    avg_loss = total_loss / max(1, batches)\n    print(f\"Epoch {epoch+1:3d}/{NUM_EPOCHS}  avg_loss={avg_loss:.4f}\")\n\n    # quick sanity decode on one batch sample from loader (no gradients)\n    model.eval()\n    with torch.no_grad():\n        Xs, targets_batch, input_lens_batch, target_lens_batch = next(iter(loader))\n        Xs = Xs.to(device)\n        input_lens_reduced = (input_lens_batch // 4).clamp(min=1).to(device)\n        logp = model(Xs)\n        preds = greedy_decode(logp, input_lens_reduced)\n        # print first sample (pred vs ground truth)\n        print(\"Sample predicted phonemes (first example):\", preds[0][:60])\n        # ground truth of first example (convert ids to symbols)\n        # need to slice out the first target from concatenated targets_batch\n        # we can reconstruct from dataset since we have batch items\n        gt_symbols = [index_to_ph[int(x)] for x in dataset[0][1][:dataset[0][3]]]\n        print(\"Example ground truth (first dataset item):\", gt_symbols)\n    model.train()\n\n# -----------------------\n# 9. Save model + metadata\n# -----------------------\nsave_path = \"sensor_phoneme_model_fixed.pth\"\ntorch.save({\n    'model_state': model.state_dict(),\n    'mean': mean,\n    'std': std,\n    'arpabet': arpabet,\n    'index_to_ph': index_to_ph\n}, save_path)\nprint(\"Model + metadata saved to\", save_path)\n\n# -----------------------\n# 10. Example inference: \"WHAT IS YOUR NAME\"\n# -----------------------\nprint(\"\\nINFERENCE EXAMPLE on generated 'WHAT IS YOUR NAME' utterance:\")\nmodel.eval()\nwith torch.no_grad():\n    # generate fresh sample (sensor) for the sentence\n    x_raw, y_ids, L = generate_sensor_sequence(\"WHAT IS YOUR NAME\")\n    airflow = x_raw[:, 0]\n    accel = x_raw[:, 1] + 1e-6\n    moisture = x_raw[:, 2]\n    r1 = airflow / accel\n    r2 = moisture / (airflow + 1e-6)\n    x_full = np.concatenate([x_raw, r1[:,None], r2[:,None]], axis=1)\n    L = min(len(x_full), MAX_SEQUENCE_LENGTH)\n    x_pad = np.pad(x_full, ((0, MAX_SEQUENCE_LENGTH - L), (0,0)), 'constant').astype(np.float32)\n    # normalize using training mean/std\n    x_tensor = (torch.tensor(x_pad, dtype=torch.float32) - mean) / std\n    x_tensor[torch.arange(MAX_SEQUENCE_LENGTH) >= L] = 0.0\n    x_tensor = x_tensor.unsqueeze(0).to(device)  # (1, T, F)\n    logp = model(x_tensor)                       # (T_reduced, 1, C)\n    input_len_reduced = torch.tensor([max(L // 4, 1)], dtype=torch.long).to(device)\n    #input_len_reduced = torch.tensor([(L // 4).clamp(min=1)]).to(device)\n    decoded = greedy_decode(logp, input_len_reduced)[0]\n    print(\"Predicted phonemes (ARPAbet):\", decoded)\n    print(\"Ground truth phonemes (ARPAbet):\", [index_to_ph[int(i)] for i in y_ids])\n\n# End of script\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T11:59:46.688579Z","iopub.execute_input":"2025-11-12T11:59:46.688853Z","iopub.status.idle":"2025-11-12T12:09:15.019023Z","shell.execute_reply.started":"2025-11-12T11:59:46.688834Z","shell.execute_reply":"2025-11-12T12:09:15.018180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# =====================================================\n# CELL 2: Load trained model + Inference + Accuracy\n# =====================================================\n\nimport torch, numpy as np, random\n\n# --- CONFIG (must match training) ---\nMAX_SEQUENCE_LENGTH = 400\nRAW_FEATURE_DIM = 5\nFEATURE_DIM = 7\nNUM_PHONEMES = 40\nBLANK_INDEX = 39\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- ARPAbet map ---\narpabet = {\n    \"AA\":0,\"AE\":1,\"AH\":2,\"AO\":3,\"AW\":4,\"AY\":5,\"B\":6,\"CH\":7,\"D\":8,\"DH\":9,\n    \"EH\":10,\"ER\":11,\"EY\":12,\"F\":13,\"G\":14,\"HH\":15,\"IH\":16,\"IY\":17,\"JH\":18,\n    \"K\":19,\"L\":20,\"M\":21,\"N\":22,\"NG\":23,\"OW\":24,\"OY\":25,\"P\":26,\"R\":27,\n    \"S\":28,\"SH\":29,\"T\":30,\"TH\":31,\"UH\":32,\"UW\":33,\"V\":34,\"W\":35,\"Y\":36,\n    \"Z\":37,\"ZH\":38,\"<BLANK>\":39\n}\nindex_to_ph = {v:k for k,v in arpabet.items()}\n\n# --- Minimal sentenceâ†’phoneme map (for accuracy) ---\nSENTENCE_PHONEMES = {\n    \"WHAT IS YOUR NAME\": [\"W\",\"AH\",\"T\",\"IH\",\"Z\",\"Y\",\"AO\",\"R\",\"N\",\"EY\",\"M\"],\n    \"SEE YOU TOMORROW\": [\"S\",\"IY\",\"Y\",\"UW\",\"T\",\"AH\",\"M\",\"AA\",\"R\",\"OW\"],\n    \"HELLO HOW ARE YOU\": [\"HH\",\"AH\",\"L\",\"OW\",\"HH\",\"AW\",\"AA\",\"R\",\"Y\",\"UW\"],\n    \"OPEN THE DOOR\": [\"OW\",\"P\",\"AH\",\"N\",\"DH\",\"AH\",\"D\",\"AO\",\"R\"],\n}\n\n# --- Model class (same arch as training) ---\nclass SensorToPhoneme(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.proj = torch.nn.Linear(FEATURE_DIM,512)\n        self.pool = torch.nn.AvgPool1d(4,4)\n        self.gru  = torch.nn.GRU(512,512,4,batch_first=True,dropout=0.3)\n        self.out  = torch.nn.Sequential(torch.nn.Linear(512,NUM_PHONEMES),\n                                        torch.nn.LogSoftmax(dim=2))\n    def forward(self,x):\n        x = torch.tanh(self.proj(x))\n        x = x.permute(0,2,1)\n        x = self.pool(x)\n        x = x.permute(0,2,1)\n        x,_ = self.gru(x)\n        x = self.out(x)\n        return x.transpose(0,1)\n\n# --- Greedy decoder ---\ndef greedy_decode(log_probs, input_lens_reduced):\n    preds = log_probs.argmax(dim=2).transpose(0,1).cpu().numpy()\n    results=[]\n    for i,Lr in enumerate(input_lens_reduced.cpu().numpy()):\n        seq = preds[i,:Lr].tolist()\n        out=[]; prev=-1\n        for s in seq:\n            if s!=prev and s!=BLANK_INDEX:\n                out.append(index_to_ph.get(int(s),\"\"))\n            prev=s\n        results.append(out)\n    return results\n\n# --- Helper: compute phoneme error rate (edit distance) ---\ndef phoneme_error_rate(pred, true):\n    m,n = len(pred), len(true)\n    dp = np.zeros((m+1,n+1),dtype=int)\n    for i in range(m+1): dp[i,0]=i\n    for j in range(n+1): dp[0,j]=j\n    for i in range(1,m+1):\n        for j in range(1,n+1):\n            cost = 0 if pred[i-1]==true[j-1] else 1\n            dp[i,j]=min(dp[i-1,j]+1, dp[i,j-1]+1, dp[i-1,j-1]+cost)\n    return dp[m,n]/max(n,1)\n\n# --- Simplified generator (for demo / unseen sentences) ---\ndef smooth_envelope(n):\n    x = np.linspace(0,np.pi,n)\n    return 0.7 + 0.3*(1-np.cos(x))/2\ndef get_manner(idx):\n    stops=[6,8,19,26,30]; affricates=[7,18]; frics=[13,14,28,29,34,37,38]\n    nasals=[21,22,23]; liquids=[20,27]; glides=[15,35,36]\n    if idx in stops: return 0\n    if idx in affricates: return 1\n    if idx in frics: return 2\n    if idx in nasals: return 3\n    if idx in liquids: return 4\n    if idx in glides: return 5\n    return 6\ndef generate_sensor_sequence(sentence):\n    ids = np.random.randint(0,38,size=random.randint(8,14))\n    seq=[]\n    for _ in ids:\n        seg=np.random.normal(0,1,(random.randint(6,12),RAW_FEATURE_DIM))\n        seq.append(seg)\n    full=np.concatenate(seq,axis=0)\n    L=min(len(full),MAX_SEQUENCE_LENGTH)\n    return full[:L], np.array(ids), L\n\n# --- Load trained model ---\ncheckpoint = torch.load(\"sensor_phoneme_model_fixed.pth\", map_location=device)\nmodel = SensorToPhoneme().to(device)\nmodel.load_state_dict(checkpoint[\"model_state\"])\nmean = checkpoint[\"mean\"].to(device)   # âœ… move to same device\nstd  = checkpoint[\"std\"].to(device)    # âœ… move to same device\nprint(\"âœ… Model loaded successfully.\")\n\n# --- Inference function with PER if possible ---\ndef infer_sentence(sentence_text):\n    model.eval()\n    with torch.no_grad():\n        x_raw,y_ids,L = generate_sensor_sequence(sentence_text)\n        airflow=x_raw[:,0]; accel=x_raw[:,1]+1e-6; moisture=x_raw[:,2]\n        r1=airflow/accel; r2=moisture/(airflow+1e-6)\n        x_full=np.concatenate([x_raw,r1[:,None],r2[:,None]],axis=1)\n        L=min(len(x_full),MAX_SEQUENCE_LENGTH)\n        x_pad=np.pad(x_full,((0,MAX_SEQUENCE_LENGTH-L),(0,0)),'constant').astype(np.float32)\n        \n        x_tensor = (torch.tensor(x_pad, device=device) - mean) / std\n\n        x_tensor[torch.arange(MAX_SEQUENCE_LENGTH)>=L]=0\n        x_tensor=x_tensor.unsqueeze(0).to(device)\n        logp=model(x_tensor)\n        Lr=torch.tensor([max(L//4,1)],dtype=torch.long).to(device)\n        pred=greedy_decode(logp,Lr)[0]\n\n        print(f\"\\nðŸ§  Sentence: {sentence_text}\")\n        print(\"Predicted phonemes:\", pred)\n        if sentence_text in SENTENCE_PHONEMES:\n            true=SENTENCE_PHONEMES[sentence_text]\n            per=phoneme_error_rate(pred,true)\n            print(\"Ground truth:\", true)\n            print(f\"Phoneme Error Rate (PER): {per:.2f}  â†’  Accuracy â‰ˆ {(1-per)*100:.1f}%\")\n\n# --- Try some examples ---\ninfer_sentence(\"WHAT IS YOUR NAME\")\ninfer_sentence(\"SEE YOU TOMORROW\")\ninfer_sentence(\"HELLO HOW ARE YOU\")\ninfer_sentence(\"OPEN THE DOOR\")\ninfer_sentence(\"NEW RANDOM SENTENCE TEST\")\ninfer_sentence(\"My name is kd\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:44:38.515176Z","iopub.execute_input":"2025-11-12T12:44:38.515731Z","iopub.status.idle":"2025-11-12T12:44:38.648919Z","shell.execute_reply.started":"2025-11-12T12:44:38.515707Z","shell.execute_reply":"2025-11-12T12:44:38.648209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(x_tensor.shape)\nprint(x_tensor)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:16:39.338448Z","iopub.execute_input":"2025-11-12T12:16:39.338721Z","iopub.status.idle":"2025-11-12T12:16:39.345421Z","shell.execute_reply.started":"2025-11-12T12:16:39.338698Z","shell.execute_reply":"2025-11-12T12:16:39.344640Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(input_lengths.shape)\nprint(input_lengths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:21:51.894978Z","iopub.execute_input":"2025-11-12T12:21:51.895249Z","iopub.status.idle":"2025-11-12T12:21:51.900245Z","shell.execute_reply.started":"2025-11-12T12:21:51.895230Z","shell.execute_reply":"2025-11-12T12:21:51.899621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}